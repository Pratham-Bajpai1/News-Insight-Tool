{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80f99bb-c4cd-4c40-a873-6bb123ae9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import pickle\n",
    "import time\n",
    "import langchain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d92ae9a-8e3b-4c1c-b173-f4d700c02345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Hugging Face tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64e311c9-a3d2-4584-85ca-5b2114fb941e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://www.moneycontrol.com/news/business/markets/wall-street-rises-as-tesla-soars-on-ai-optimism-11351111.html\",\n",
    "    \"https://www.moneycontrol.com/news/business/tata-motors-launches-punch-icng-price-starts-at-rs-7-1-lakh-11098751.html\"\n",
    "]\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8860592c-9f5e-4830-9f89-7debc1e1ffb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RecursiveCharacterTextSplitter' object has no attribute 'split_document'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      3\u001b[0m recurs_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[0;32m      4\u001b[0m     separators \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      5\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m      6\u001b[0m     chunk_overlap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mrecurs_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_document\u001b[49m(data)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mlen\u001b[39m(chunks)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RecursiveCharacterTextSplitter' object has no attribute 'split_document'"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recurs_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \"],\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "chunks = recurs_splitter.split_document(data)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f2697a-085b-4e9c-9fa5-6458f5a0a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1012e9b0-1e35-4a80-bc86-f0480072c7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206c8230d3524e8f8204334acad73562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prath\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3afb40a35f142d7a2f3a762231b9b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92af51a6baf04f33ad15165ab0c6723d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99df6695e19648f5ada739da513a899a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "951f47e3-fbd7-403f-8c78-325c48ef8829",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input must be either a string or a list of strings",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m chunk\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be either a string or a list of strings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Tokenize each chunk and get embeddings\u001b[39;00m\n\u001b[0;32m     13\u001b[0m chunk_embeddings\u001b[38;5;241m.\u001b[39mextend([\n\u001b[0;32m     14\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmean(\n\u001b[0;32m     15\u001b[0m         model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer(sub_chunk, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sub_chunk \u001b[38;5;129;01min\u001b[39;00m chunks\n\u001b[0;32m     19\u001b[0m ])\n",
      "\u001b[1;31mValueError\u001b[0m: Input must be either a string or a list of strings"
     ]
    }
   ],
   "source": [
    "# Get embeddings for each chunk\n",
    "chunk_embeddings = []\n",
    "for chunk in docs:\n",
    "    # Ensure chunk is either a string or a list of strings\n",
    "    if isinstance(chunk, str):\n",
    "        chunks = [chunk]\n",
    "    elif isinstance(chunk, list) and all(isinstance(sub_chunk, str) for sub_chunk in chunk):\n",
    "        chunks = chunk\n",
    "    else:\n",
    "        raise ValueError(\"Input must be either a string or a list of strings\")\n",
    "\n",
    "    # Tokenize each chunk and get embeddings\n",
    "    chunk_embeddings.extend([\n",
    "        torch.mean(\n",
    "            model(**tokenizer(sub_chunk, return_tensors=\"pt\", padding=True, truncation=True, max_length=512))[\"last_hidden_state\"],\n",
    "            dim=1\n",
    "        ).squeeze().numpy()\n",
    "        for sub_chunk in chunks\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dd719-2f69-425e-86da-c7dbdd56727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified code for creating embeddings of text chunks\n",
    "# Get embeddings for each chunk\n",
    "chunk_embeddings = get_embeddings(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec4301-bda3-48c3-bc5f-c95438854ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda90341-0584-4a8b-bdf8-1a9be4466218",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer(\"all-mpnet-base-v2\") # calculate ecludiean distance\n",
    "vectors = encoder.encode(docs) # create embeddings/vector of each text in df\n",
    "vectors.shape # each vector length is 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be98160e-818c-4daa-adeb-39563464ac69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
